\documentclass[11pt]{report}
\usepackage{./assignment}
\usepackage{diagbox}
\usepackage{enumitem}
\usepackage{stmaryrd}
\usepackage{cprotect}
\usepackage{graphicx}
\usepackage{subfigure}

\input{./Definitions}


\lecturenumber{1}       % assignment number
\duedate{17:00, Mar 6, 2018}

% Fill in your name and email address
\stuinfo{George Maratos Ashwani Khemani}{gmarat2@uic.edu, akhema2@uic.edu}

\graphicspath{{./}{./Figures/}}


\begin{document}
	
	\maketitle
	\section*{1a Solution}

	We can simplify $\log p(y^t, X^t)$ as follows:
	\begin{align}
	\log \frac{1}{Z_X} \exp \rbr{\sum_{s=1}^m \inner{\wvec_{y_s}}{\xvec_s} + \sum_{s=1}^{m-1} T_{y_s, y_{s+1}}} &= \sum_{s=1}^m \inner{\wvec_{y_s}}{\xvec_s} +  \sum_{s=1}^{m-1} T_{y_s, y_{s+1}} - \log Z_x 
	\end{align}


	The $\grad_{\wvec_y} \sum_{s=1}^m \inner{\wvec_{y_s}}{\xvec_s} + \sum_{s=1}^{m-1} T_{y_s, y_{s+1}}$ is the following:

	\begin{align}
	\sum_{s=1}^m \sembrack{y_s = y}x_s^t
	\end{align}

	This is because while taking the derivative of a dot product involving $w_y$, $X_s$ will only appear whenever
	$y_s = y$. And, the sum of transitions will disappear because it does not depend on w.\\

	The $\grad_{\wvec_y} \log Z_X$ is computed via the chain rule in the following way:

	\begin{align}
	\grad_{\wvec_y} \log Z_X &= \frac{ \sum_{\hat{\yvec} \in \Ycal^m} \exp \rbr{\sum_{s=1}^m \inner{\wvec_{\yhat_s}}{\xvec_s} + \sum_{s=1}^{m-1} T_{\yhat_s, \yhat_{s+1}}} }
	{\sum_{\hat{\yvec} \in \Ycal^m} \exp \rbr{\sum_{s=1}^m \inner{\wvec_{\yhat_s}}{\xvec_s} + \sum_{s=1}^{m-1} T_{\yhat_s, \yhat_{s+1}}}} * \sum_{s=1}^m \sembrack{y_s = y}x_s^t
	\end{align}

	This can be further reduced, by substituting $p(y|X)$ into equation 3 and rearanging the sums, in the following way:

	\begin{align}
	\sum_{\hat{\yvec} \in \Ycal^m} p(y|X) \sum_{s=1}^m \sembrack{y_s = y}x_s^t &= \sum_{s=1}^m  \sum_{\hat{\yvec} \in \Ycal^m}   p(y|X)\sembrack{y_s = y}x_s^t
	\end{align}

	Finally, we recognize that the inner summation is a marginalization over $y$ except the label we are taking the gradient against. Therefore we can further reduce the equation to:

	\begin{align}
	 \sum_{s=1}^m {p(y_s=y | X^t))}x_s^t
	\end{align}

	Using equations 2 and 5, the gradient is 
	\begin{align}
		\grad_{\wvec_y} \log p(\yvec^t|X^t) &= \sum_{s=1}^m (\sembrack{y^t_s = y} - p(y_s = y | X^t)) \xvec^t_s,
	\end{align}
	


	Now computing gradient for T


	The $\grad_{T_ij} \sum_{s=1}^m \inner{\wvec_{y_s}}{\xvec_s} + \sum_{s=1}^{m-1} T_{y_s, y_{s+1}}$ is the following:
	\begin{align}
	\sum_{s=1}^{m-1} \sembrack{y_s = i, y_{s+1} = j}
	\end{align}

	This is because when we differentiate with respect to $T_{ij}$, the only terms that remain are the ones that specify a transition between i and j. There is only a single weight that lies at this transition point so when we differentiate it, it becomes 1. Also, the dot product goes away because it does not depend on T.

	The $\grad_{T_{ij}} \log Z_X$ is computed via the chain rule in the following way:

	\begin{align}
	\grad_{T_{ij}} \log Z_X &= \frac{ \sum_{\hat{\yvec} \in \Ycal^m} \exp \rbr{\sum_{s=1}^m \inner{\wvec_{\yhat_s}}{\xvec_s} + \sum_{s=1}^{m-1} T_{\yhat_s, \yhat_{s+1}}} }
	{\sum_{\hat{\yvec} \in \Ycal^m} \exp \rbr{\sum_{s=1}^m \inner{\wvec_{\yhat_s}}{\xvec_s} + \sum_{s=1}^{m-1} T_{\yhat_s, \yhat_{s+1}}}} * \sum_{s=1}^{m-1} \sembrack{y_s = i, y_{s+1} = j}
	\end{align}


	This can be further reduced, by substituting $p(y|X)$ into equation 8 and rearanging the sums, in the following way:

	\begin{align}
	\sum_{\hat{\yvec} \in \Ycal^m} p(y|X) \sum_{s=1}^{m-1} \sembrack{y_s = i, y_{s+1} = j} &= \sum_{s=1}^{m-1}  \sum_{\hat{\yvec} \in \Ycal^m}   p(y|X)\sembrack{y_s = i, y_{s+1} = j}
	\end{align}


	Finally, we recognize that the inner summation is a marginalization over $y$ except the label we are taking the gradient against. Therefore we can further reduce the equation to:

	\begin{align}
	 \sum_{s=1}^{m-1} {p(y_s=i, y_{s+1}=j | X^t)}
	\end{align}


	Using equations 7 and 9 the gradient is:

	\begin{align}
	\sum_{s=1}^{m-1} \sembrack{y_s = i, y_{s+1} = j} - p(y_s=i, y_{s+1}=j | X^t)
	\end{align}
	

\section*{1b Solution}

The features will take the following form. If they are a feature that depends on the letter's label, they will look as follows:


	\begin{align}
	x_i \sembrack{y_i=j}
	\end{align}

where $i$ will be between $1...m$ and $j$ will be between $1...26$. If the feature depends on the transition between two letters then that feature will take the following form. 


	\begin{align}
	\sembrack{y_i = k, y_i+1 = z}
	\end{align}

where $i$ is between $1...m$ and $k, z$ are between $1...26$

The gradient of $\log Z_x$ is described in equations 3 and 8. The feature function $\phi(x)$ will "place" the input in specific positions so that the correct dot product and transitions are computed. 
For example, the feature function enables the selection of $w_7$ to be used in $\inner{x_i}{w_7}$ when $y_i = 7$. \\

Considering features that depend on the label first. The exectation of some features with respect to $p(\yvec|X)$ will be:

	\begin{align}
	\sum_{s=1}^{m} p(\yvec|X)\phi(X)
	\end{align}

And this is equivalent to:

	\begin{align}
	\sum_{s=1}^{m} p(\yvec|X^t)x_s \sembrack{y_s=y}
	\end{align}

The feature function (via the indicator) will only be non-zero for terms s.t. $y_s=y$ and the others will go to zero. Therefore we can say that the above is equivalent to equation 3 because the 
$p(\yvec|X)$ term will become $p(y_s=y|X)$ via marginalization, and $x_s$ will be selected by the feature function. The gradient with respect to $T$ has a similar argument. But instead what will be 
important is the feature $\sembrack{y_s = i, y_{s+1} = j}$ which will marginalize $p(\yvec|X)$ to $p(y_s=i,y_{s+1}=j)$. 
	
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 Maximum Objective Value for 1C & 200.33257896998455\\
 \hline
 Average Log Probability for 2A & -31.28\\
 \hline
 Optimal Objective Value for 2B & 3701.15\\
 \hline

\end{tabular}
\end{center}

\begin{figure}[t!]
\begin{minipage}[b]{0.50\textwidth} 
\centering 	
\vspace{-0.6em} 
\includegraphics[width=9cm]{CRF_letter_wise.png}
\vspace{0.6em} 
\end{minipage} 
~~~ 
\begin{minipage}[b]{0.50\textwidth} 
\centering 	
\vspace{-0.6em} 
\includegraphics[width=9cm]{SVM-HMM_word_wise.png}
\vspace{0.6em} 
\end{minipage} 
~~~ 
\begin{minipage}[b]{0.50\textwidth} 
\centering 	
\vspace{-0.6em} 
\includegraphics[width=9cm]{SVM-MC_letter_wise.png}
\vspace{0.6em} 
\end{minipage} 
~~~ 
\begin{minipage}[b]{0.50\textwidth} 
\centering 	
\vspace{-0.6em} 
\includegraphics[width=9cm]{CRF_word_wise.png}
\vspace{0.6em} 
\end{minipage} 
~~~ 
\begin{minipage}[b]{0.50\textwidth} 
\centering 	
\vspace{-0.6em} 
\includegraphics[width=9cm]{SVM-HMM_letter_wise.png}
\vspace{0.6em} 
\end{minipage} 
~~~ 
\begin{minipage}[b]{0.50\textwidth} 
\centering 	
\vspace{-0.6em} 
\includegraphics[width=9cm]{SVM-MC_word_wise.png}
\vspace{0.6em} 
\end{minipage} 
\end{figure}

\section*{Observation for question 3} 

In general , as the value of C increase the letter wise accuracy and word wise accuracy increase for the given models : SVM-MC , SVM-HMM , CRF .
We observed that the word wise accuracy was substantially lower than letter wise accuracy for all the given models . This behavior is as expected as word wise classification is a harder task as compared 
to letter wise classification .Both CRF and SVM-HMM perform better as compared to SVM-MC both word wise and letter wise . The performance of CRF is generally better then both the SVM models .
The SVM-struct performs slightly better than the CRF model . 

\begin{figure}[t!]
\begin{minipage}[b]{0.50\textwidth}
\centering
\vspace{-0.6em}
\includegraphics[width=9cm]{SVM_letter_wise.png}
\vspace{0.6em}
\end{minipage}

\begin{minipage}[b]{0.50\textwidth}
\centering
\vspace{-0.6em}
\includegraphics[width=9cm]{SVM_word_wise.png}
\vspace{0.6em}
\end{minipage}
\end{figure}

\begin{figure}[t!]
\begin{minipage}[b]{0.50\textwidth}
\centering
\vspace{-0.6em}
\includegraphics[width=9cm]{CRF_letter_wise_t.png}
\vspace{0.6em}
\end{minipage}

\begin{minipage}[b]{0.50\textwidth}
\centering
\vspace{-0.6em}
\includegraphics[width=9cm]{CRF_word_wise_t.png}
\vspace{0.6em}
\end{minipage}
\end{figure}


\section*{Observation for question 4}
In general , as the number of transformation increase , the letter wise / word wise accuracy decrease for the given models : SVM-MC , SVM-HMM , CRF ,
This is as expected because once we tamper the data with increasing number of transformations , the performance of the model should decrease .
CRF performs better than SVM-MC and the effect of tampering is not seen as much for CRF , but the same is not true for SVM-MC as 
it performance gets worse with increase in number of transformations .

\end{document}


